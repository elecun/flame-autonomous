{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     _squared_sum_channels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     49\u001b[0m     _total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 50\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43m_sum_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_total_batches\u001b[49m\n\u001b[0;32m     51\u001b[0m std \u001b[38;5;241m=\u001b[39m (_squared_sum_channels \u001b[38;5;241m/\u001b[39m _squared_sum_channels \u001b[38;5;241m-\u001b[39m mean \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(_sum_channels, _squared_sum_channels, _total_batches)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATA_BATCH_SIZE = 10\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.01\n",
    "GRAD_CLIP = 0.12\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "DATASET_PATH = pathlib.Path(os.path.dirname(os.path.realpath('__file__'))).parent.parent / \"dataset\" / \"SDD\" / \"luxteel\" / \"resnet_dataset\"\n",
    "\n",
    "TRAIN_DATASET_PATH = pathlib.Path(os.path.dirname(os.path.realpath('__file__'))).parent.parent / \"dataset\" / \"SDD\" / \"luxteel\" / \"resnet_dataset\" / \"train\" / \"normal\"\n",
    "TEST_DATASET_PATH = pathlib.Path(os.path.dirname(os.path.realpath('__file__'))).parent.parent / \"dataset\" / \"SDD\" / \"luxteel\" / \"resnet_dataset\" / \"test\"\n",
    "\n",
    "#defect_image_files = os.listdir(DEFECT_DATASET_PATH.as_posix())\n",
    "normal_image_files = os.listdir(TRAIN_DATASET_PATH.as_posix())\n",
    "\n",
    "# random shuffle\n",
    "#random.shuffle(defect_image_files)\n",
    "#random.shuffle(normal_image_files)\n",
    "\n",
    "# split dataset\n",
    "trainset_rate = 0.7\n",
    "#defect_train_size = int(trainset_rate*len(defect_image_files))\n",
    "normal_train_size = int(trainset_rate*len(normal_image_files))\n",
    "\n",
    "\n",
    "dataset = ImageFolder(TRAIN_DATASET_PATH, transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=DATA_BATCH_SIZE)\n",
    "\n",
    "# calc mean and std for image normalization\n",
    "_sum_channels = 0\n",
    "_squared_sum_channels =0\n",
    "_total_batches = 0\n",
    "\n",
    "for data, _ in tqdm(dataloader):\n",
    "    _sum_channels += torch.mean(data, dim=[0,2,3]) # calc mean for each channels (dim=Batch, Channel, Height, Width)\n",
    "    _squared_sum_channels += torch.mean(data**2, dim=[0,2,3])\n",
    "    _total_batches += 1\n",
    "mean = _sum_channels / _total_batches\n",
    "std = (_squared_sum_channels / _squared_sum_channels - mean ** 2) ** 0.5\n",
    "\n",
    "print(_sum_channels, _squared_sum_channels, _total_batches)\n",
    "print(f\"Mean : {mean}, Std:{std}\")\n",
    "\n",
    "# normalization(0~1) of dataset images\n",
    "stats = (tuple(mean.tolist()), tuple(std.tolist()))\n",
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats, inplace=True)])\n",
    "\n",
    "train_dataset = ImageFolder((DATASET_PATH/\"train\").as_posix(), train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=DATA_BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# device selection function (GPU, CPU, MPS for MacOS)\n",
    "def get_device_use():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_device_use()\n",
    "print(f\"Selected Device : {device}\")\n",
    ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "# transfer data into the selected device\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dataloader, device) -> None:\n",
    "        self.__dataloader = dataloader\n",
    "        self.__device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.__dataloader:\n",
    "            yield to_device(b, self.__device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.__dataloader)\n",
    "    \n",
    "train_dataloader = DeviceDataLoader(train_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 3, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# convolutional auto encoder model impl.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # encoder layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1) # depth 3 -> 16\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=4, kernel_size=3, padding=1) # depth 16 -> 4\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # decoder layer\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        # encode\n",
    "        x = F.relu(self.conv1(x_in))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #decode\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        images, _ = data\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
