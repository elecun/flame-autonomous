{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Surface Defect Detection using Convolutional AutoEncoder\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pathlib\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:07<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.0587, 15.0587, 15.0587]) tensor([4.7590, 4.7590, 4.7590]) 51\n",
      "Mean : tensor([0.2953, 0.2953, 0.2953]), Std:tensor([0.9554, 0.9554, 0.9554])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Train/Test Data Preparation\n",
    "'''\n",
    "# normal dataset path\n",
    "TRAIN_DATASET_PATH = pathlib.Path(os.path.dirname(os.path.realpath('__file__'))).parent.parent / \"dataset\" / \"SDD\" / \"luxteel\" / \"ae_dataset\" / \"train\"\n",
    "TEST_DATASET_PATH = pathlib.Path(os.path.dirname(os.path.realpath('__file__'))).parent.parent / \"dataset\" / \"SDD\" / \"luxteel\" / \"ae_dataset\" / \"test\"\n",
    "\n",
    "# set dataloader\n",
    "DATA_BATCH_SIZE = 10\n",
    "dataset = ImageFolder(TRAIN_DATASET_PATH, transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=DATA_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# calculate mean and std for normalization\n",
    "_sum_channels = 0\n",
    "_squared_sum_channels = 0\n",
    "_total_batches = 0\n",
    "\n",
    "for data, _ in tqdm(dataloader):\n",
    "    _sum_channels += torch.mean(data, dim=[0,2,3]) # calc mean for each channels (dim=Batch, Channel, Height, Width)\n",
    "    _squared_sum_channels += torch.mean(data**2, dim=[0,2,3])\n",
    "    _total_batches += 1\n",
    "\n",
    "mean = _sum_channels / _total_batches\n",
    "std = (_squared_sum_channels / _squared_sum_channels - mean ** 2) ** 0.5\n",
    "print(_sum_channels, _squared_sum_channels, _total_batches)\n",
    "print(f\"Mean : {mean}, Std:{std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[1;32mIn[8], line 64\u001b[0m, in \u001b[0;36mConvAutoencoder.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m---> 64\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     reconst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoding)\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mConvAutoencoder.encoder\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     31\u001b[0m relu1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(conv1) \u001b[38;5;66;03m# 640x400x128\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m pool1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool1\u001b[49m(relu1) \u001b[38;5;66;03m# 320x200x128\u001b[39;00m\n\u001b[0;32m     34\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_2(pool1)\n",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConvAutoencoder' object has no attribute 'pool1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encoding, reconst\n\u001b[0;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m ConvAutoencoder(channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elecu\\dev\\flame-autonomous\\venv\\Lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convolutional Auto Encoder\n",
    "'''\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv_1 = nn.Conv2d(in_channels=channels, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1) # same dimension?\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=2, padding='same')\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=2, padding='same')\n",
    "        \n",
    "        # Decoder\n",
    "        self.upscale_1 = nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.upscale_2 = nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.upscale_3 = nn.ConvTranspose2d(in_channels=128, out_channels=channels, kernel_size=3, stride=2, padding=2)\n",
    "        \n",
    "    \n",
    "    # encoder part        \n",
    "    def encoder(self, image): # 640x400x3\n",
    "        conv1 = self.conv_1(image)\n",
    "        relu1 = F.relu(conv1) # 640x400x128\n",
    "        pool1 = self.pool1(relu1) # 320x200x128\n",
    "  \n",
    "        conv2 = self.conv_2(pool1)\n",
    "        relu2 = F.relu(conv2) # 320x200x64\n",
    "        pool2 = self.pool1(relu2) #160x100x64\n",
    "        \n",
    "        conv3 = self.conv_3(pool2)\n",
    "        relu3 = F.relu(conv3) # 160x100x32\n",
    "        pool3 = self.pool2(relu3) # 80x50x32\n",
    "        \n",
    "        #pool3 = pool3.view([image.size(0), 128, 80, 40]).cuda()\n",
    "        return pool3\n",
    "    \n",
    "    # decoder part\n",
    "    def decoder(self, encoding):\n",
    "        up1 = self.upscale_1(encoding) # 160x100x64\n",
    "        up_relu1 = F.relu(up1)\n",
    "        \n",
    "        up2 = self.upscale_2(up_relu1) # 320x200x128\n",
    "        up_relu2 = F.relu(up2)\n",
    "        \n",
    "        up3 = self.upscale_3(up_relu2) # 640x400x3\n",
    "        up_relu3 = F.relu(up3)\n",
    "        \n",
    "        return up_relu3\n",
    "        \n",
    "        # logits = self.conv(up_relu3)\n",
    "        # logits = F.sigmoid(logits)\n",
    "        # logits = logits.view([encoding.size(0), 1, 28, 28]).cuda()\n",
    "        # return logits\n",
    "    \n",
    "    def forward(self, image):\n",
    "        encoding = self.encoder(image)\n",
    "        reconst = self.decoder(encoding)\n",
    "        return encoding, reconst\n",
    "    \n",
    "    \n",
    "model = ConvAutoencoder(channels=3)\n",
    "summary(model, input_size=(3, 640, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Phase\n",
    "'''\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(data_loader, size, model, criterion, optimizer, num_epochs=20):\n",
    "\tprint('Start training')\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "\t\ttloss = 0.0\n",
    "\t\tfor data in data_loader:\n",
    "\t\t\tinputs, _ = data\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tencoding, logits = model(Variable(inputs.cuda()))\n",
    "\t\t\tloss = criterion(logits, Variable(inputs.cuda()))\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttloss += loss.data[0]\n",
    "\t\tepoch_loss = tloss/size\n",
    "\t\tprint('Epoch loss: {:4f}'.format(epoch_loss))\n",
    "\tprint('Complete training')\n",
    "\treturn model\n",
    "\n",
    "# create model instance and upload into cuda device\n",
    "model = ConvAutoencoder().cuda()\n",
    "\n",
    "# \n",
    "criterion = nn.BCELoss()\n",
    "size = len(dataset)\n",
    "\n",
    "optimizer_fn = optim.Adam\n",
    "optimizer = optimizer_fn(model.parameters(), lr=LEARNING_RATE)\n",
    "model = train(dataloader, size, model, criterion, optimizer, num_epochs=EPOCH)\n",
    "\n",
    "# test_image = random.choice(test_data)\n",
    "# test_image = Variable(test_image[0].unsqueeze(0).cuda())\n",
    "# _, out = model(test_image)\n",
    "\n",
    "torchvision.utils.save_image(test_image.data, 'in.png')\n",
    "torchvision.utils.save_image(out.data, 'out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set dataloader\n",
    "DATA_BATCH_SIZE = 10\n",
    "\n",
    "dataset = ImageFolder(TRAIN_DATASET_PATH, transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=DATA_BATCH_SIZE)\n",
    "\n",
    "\n",
    "# calc mean and std for image normalization\n",
    "_sum_channels = 0\n",
    "_squared_sum_channels = 0\n",
    "_total_batches = 0\n",
    "\n",
    "for data, _ in tqdm(dataloader):\n",
    "    _sum_channels += torch.mean(data, dim=[0,2,3]) # calc mean for each channels (dim=Batch, Channel, Height, Width)\n",
    "    _squared_sum_channels += torch.mean(data**2, dim=[0,2,3])\n",
    "    _total_batches += 1\n",
    "\n",
    "mean = _sum_channels / _total_batches\n",
    "std = (_squared_sum_channels / _squared_sum_channels - mean ** 2) ** 0.5\n",
    "print(_sum_channels, _squared_sum_channels, _total_batches)\n",
    "print(f\"Mean : {mean}, Std:{std}\")\n",
    "\n",
    "\n",
    "# normalization(0~1) of dataset images\n",
    "# normalization이 필요할까 모르겠다\n",
    "stats = (tuple(mean.tolist()), tuple(std.tolist()))\n",
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats, inplace=True)])\n",
    "\n",
    "train_dataset = ImageFolder(TRAIN_DATASET_PATH.as_posix(), train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=DATA_BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
